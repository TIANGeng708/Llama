{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 43800,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01141552511415525,
      "grad_norm": 1.3151443004608154,
      "learning_rate": 1.9771689497716898e-05,
      "loss": 2.6653,
      "step": 500
    },
    {
      "epoch": 0.0228310502283105,
      "grad_norm": 1.3846060037612915,
      "learning_rate": 1.954337899543379e-05,
      "loss": 2.6034,
      "step": 1000
    },
    {
      "epoch": 0.03424657534246575,
      "grad_norm": 1.444757342338562,
      "learning_rate": 1.9315525114155254e-05,
      "loss": 2.57,
      "step": 1500
    },
    {
      "epoch": 0.045662100456621,
      "grad_norm": 1.553785800933838,
      "learning_rate": 1.9087214611872146e-05,
      "loss": 2.5679,
      "step": 2000
    },
    {
      "epoch": 0.05707762557077625,
      "grad_norm": 1.190148949623108,
      "learning_rate": 1.8858904109589042e-05,
      "loss": 2.5368,
      "step": 2500
    },
    {
      "epoch": 0.0684931506849315,
      "grad_norm": 1.4452345371246338,
      "learning_rate": 1.863059360730594e-05,
      "loss": 2.559,
      "step": 3000
    },
    {
      "epoch": 0.07990867579908675,
      "grad_norm": 1.2388166189193726,
      "learning_rate": 1.8402283105022834e-05,
      "loss": 2.5432,
      "step": 3500
    },
    {
      "epoch": 0.091324200913242,
      "grad_norm": 1.1264398097991943,
      "learning_rate": 1.8174429223744294e-05,
      "loss": 2.5388,
      "step": 4000
    },
    {
      "epoch": 0.10273972602739725,
      "grad_norm": 1.2158843278884888,
      "learning_rate": 1.7946118721461187e-05,
      "loss": 2.5247,
      "step": 4500
    },
    {
      "epoch": 0.1141552511415525,
      "grad_norm": 0.7957170605659485,
      "learning_rate": 1.771826484018265e-05,
      "loss": 2.5369,
      "step": 5000
    },
    {
      "epoch": 0.12557077625570776,
      "grad_norm": 1.0614144802093506,
      "learning_rate": 1.7489954337899543e-05,
      "loss": 2.5586,
      "step": 5500
    },
    {
      "epoch": 0.136986301369863,
      "grad_norm": 1.3078621625900269,
      "learning_rate": 1.726164383561644e-05,
      "loss": 2.5358,
      "step": 6000
    },
    {
      "epoch": 0.14840182648401826,
      "grad_norm": 1.6278350353240967,
      "learning_rate": 1.70337899543379e-05,
      "loss": 2.5407,
      "step": 6500
    },
    {
      "epoch": 0.1598173515981735,
      "grad_norm": 1.9384424686431885,
      "learning_rate": 1.6805479452054795e-05,
      "loss": 2.5136,
      "step": 7000
    },
    {
      "epoch": 0.17123287671232876,
      "grad_norm": 1.000634789466858,
      "learning_rate": 1.657716894977169e-05,
      "loss": 2.517,
      "step": 7500
    },
    {
      "epoch": 0.182648401826484,
      "grad_norm": 1.1523184776306152,
      "learning_rate": 1.6348858447488584e-05,
      "loss": 2.545,
      "step": 8000
    },
    {
      "epoch": 0.19406392694063926,
      "grad_norm": 1.3953224420547485,
      "learning_rate": 1.612054794520548e-05,
      "loss": 2.5087,
      "step": 8500
    },
    {
      "epoch": 0.2054794520547945,
      "grad_norm": 1.018192172050476,
      "learning_rate": 1.5892237442922376e-05,
      "loss": 2.5362,
      "step": 9000
    },
    {
      "epoch": 0.21689497716894976,
      "grad_norm": 1.171452283859253,
      "learning_rate": 1.5663926940639272e-05,
      "loss": 2.5484,
      "step": 9500
    },
    {
      "epoch": 0.228310502283105,
      "grad_norm": 1.3134640455245972,
      "learning_rate": 1.5435616438356164e-05,
      "loss": 2.5429,
      "step": 10000
    },
    {
      "epoch": 0.23972602739726026,
      "grad_norm": 1.2987244129180908,
      "learning_rate": 1.520730593607306e-05,
      "loss": 2.517,
      "step": 10500
    },
    {
      "epoch": 0.2511415525114155,
      "grad_norm": 2.091269016265869,
      "learning_rate": 1.4978995433789956e-05,
      "loss": 2.5215,
      "step": 11000
    },
    {
      "epoch": 0.2625570776255708,
      "grad_norm": 1.6012133359909058,
      "learning_rate": 1.475068493150685e-05,
      "loss": 2.5108,
      "step": 11500
    },
    {
      "epoch": 0.273972602739726,
      "grad_norm": 1.3596328496932983,
      "learning_rate": 1.4522374429223745e-05,
      "loss": 2.5247,
      "step": 12000
    },
    {
      "epoch": 0.2853881278538813,
      "grad_norm": 1.4388612508773804,
      "learning_rate": 1.429406392694064e-05,
      "loss": 2.5031,
      "step": 12500
    },
    {
      "epoch": 0.2968036529680365,
      "grad_norm": 1.299344539642334,
      "learning_rate": 1.4065753424657535e-05,
      "loss": 2.5358,
      "step": 13000
    },
    {
      "epoch": 0.3082191780821918,
      "grad_norm": 1.4562537670135498,
      "learning_rate": 1.3837899543378997e-05,
      "loss": 2.5182,
      "step": 13500
    },
    {
      "epoch": 0.319634703196347,
      "grad_norm": 1.3148980140686035,
      "learning_rate": 1.3609589041095891e-05,
      "loss": 2.5324,
      "step": 14000
    },
    {
      "epoch": 0.3310502283105023,
      "grad_norm": 1.9310884475708008,
      "learning_rate": 1.3381278538812787e-05,
      "loss": 2.5245,
      "step": 14500
    },
    {
      "epoch": 0.3424657534246575,
      "grad_norm": 1.4969720840454102,
      "learning_rate": 1.315296803652968e-05,
      "loss": 2.5322,
      "step": 15000
    },
    {
      "epoch": 0.3538812785388128,
      "grad_norm": 1.2639079093933105,
      "learning_rate": 1.2924657534246576e-05,
      "loss": 2.5256,
      "step": 15500
    },
    {
      "epoch": 0.365296803652968,
      "grad_norm": 1.1381916999816895,
      "learning_rate": 1.2696347031963472e-05,
      "loss": 2.5245,
      "step": 16000
    },
    {
      "epoch": 0.3767123287671233,
      "grad_norm": 1.634807825088501,
      "learning_rate": 1.2468036529680368e-05,
      "loss": 2.5189,
      "step": 16500
    },
    {
      "epoch": 0.3881278538812785,
      "grad_norm": 1.165611743927002,
      "learning_rate": 1.223972602739726e-05,
      "loss": 2.5143,
      "step": 17000
    },
    {
      "epoch": 0.3995433789954338,
      "grad_norm": 1.9227540493011475,
      "learning_rate": 1.2011415525114157e-05,
      "loss": 2.5236,
      "step": 17500
    },
    {
      "epoch": 0.410958904109589,
      "grad_norm": 1.1458616256713867,
      "learning_rate": 1.1783561643835617e-05,
      "loss": 2.5265,
      "step": 18000
    },
    {
      "epoch": 0.4223744292237443,
      "grad_norm": 1.081174373626709,
      "learning_rate": 1.1555251141552513e-05,
      "loss": 2.5164,
      "step": 18500
    },
    {
      "epoch": 0.4337899543378995,
      "grad_norm": 1.6821699142456055,
      "learning_rate": 1.1326940639269409e-05,
      "loss": 2.5279,
      "step": 19000
    },
    {
      "epoch": 0.4452054794520548,
      "grad_norm": 1.2844908237457275,
      "learning_rate": 1.1098630136986301e-05,
      "loss": 2.5055,
      "step": 19500
    },
    {
      "epoch": 0.45662100456621,
      "grad_norm": 1.471086025238037,
      "learning_rate": 1.0870776255707765e-05,
      "loss": 2.5199,
      "step": 20000
    },
    {
      "epoch": 0.4680365296803653,
      "grad_norm": 1.6320240497589111,
      "learning_rate": 1.0642922374429225e-05,
      "loss": 2.5152,
      "step": 20500
    },
    {
      "epoch": 0.4794520547945205,
      "grad_norm": 1.7649928331375122,
      "learning_rate": 1.041461187214612e-05,
      "loss": 2.4968,
      "step": 21000
    },
    {
      "epoch": 0.4908675799086758,
      "grad_norm": 0.948176383972168,
      "learning_rate": 1.0186301369863013e-05,
      "loss": 2.5229,
      "step": 21500
    },
    {
      "epoch": 0.502283105022831,
      "grad_norm": 1.4560186862945557,
      "learning_rate": 9.95799086757991e-06,
      "loss": 2.5176,
      "step": 22000
    },
    {
      "epoch": 0.5136986301369864,
      "grad_norm": 1.3319660425186157,
      "learning_rate": 9.729680365296804e-06,
      "loss": 2.5301,
      "step": 22500
    },
    {
      "epoch": 0.5251141552511416,
      "grad_norm": 1.3525234460830688,
      "learning_rate": 9.5013698630137e-06,
      "loss": 2.5336,
      "step": 23000
    },
    {
      "epoch": 0.5365296803652968,
      "grad_norm": 1.55711030960083,
      "learning_rate": 9.27351598173516e-06,
      "loss": 2.5134,
      "step": 23500
    },
    {
      "epoch": 0.547945205479452,
      "grad_norm": 1.0013631582260132,
      "learning_rate": 9.045205479452056e-06,
      "loss": 2.5271,
      "step": 24000
    },
    {
      "epoch": 0.5593607305936074,
      "grad_norm": 1.476867437362671,
      "learning_rate": 8.81689497716895e-06,
      "loss": 2.5064,
      "step": 24500
    },
    {
      "epoch": 0.5707762557077626,
      "grad_norm": 1.6593352556228638,
      "learning_rate": 8.589041095890412e-06,
      "loss": 2.4874,
      "step": 25000
    },
    {
      "epoch": 0.5821917808219178,
      "grad_norm": 1.2006417512893677,
      "learning_rate": 8.360730593607306e-06,
      "loss": 2.5148,
      "step": 25500
    },
    {
      "epoch": 0.593607305936073,
      "grad_norm": 1.7505563497543335,
      "learning_rate": 8.1324200913242e-06,
      "loss": 2.4952,
      "step": 26000
    },
    {
      "epoch": 0.6050228310502284,
      "grad_norm": 1.0151028633117676,
      "learning_rate": 7.904109589041097e-06,
      "loss": 2.5273,
      "step": 26500
    },
    {
      "epoch": 0.6164383561643836,
      "grad_norm": 1.233444094657898,
      "learning_rate": 7.675799086757991e-06,
      "loss": 2.5178,
      "step": 27000
    },
    {
      "epoch": 0.6278538812785388,
      "grad_norm": 1.3314096927642822,
      "learning_rate": 7.447488584474887e-06,
      "loss": 2.5,
      "step": 27500
    },
    {
      "epoch": 0.639269406392694,
      "grad_norm": 1.3448126316070557,
      "learning_rate": 7.219178082191781e-06,
      "loss": 2.5046,
      "step": 28000
    },
    {
      "epoch": 0.6506849315068494,
      "grad_norm": 1.3512020111083984,
      "learning_rate": 6.990867579908677e-06,
      "loss": 2.5192,
      "step": 28500
    },
    {
      "epoch": 0.6621004566210046,
      "grad_norm": 2.1957480907440186,
      "learning_rate": 6.762557077625571e-06,
      "loss": 2.5084,
      "step": 29000
    },
    {
      "epoch": 0.6735159817351598,
      "grad_norm": 1.6471315622329712,
      "learning_rate": 6.5342465753424666e-06,
      "loss": 2.5164,
      "step": 29500
    },
    {
      "epoch": 0.684931506849315,
      "grad_norm": 1.6113377809524536,
      "learning_rate": 6.3063926940639275e-06,
      "loss": 2.5166,
      "step": 30000
    },
    {
      "epoch": 0.6963470319634704,
      "grad_norm": 2.687396287918091,
      "learning_rate": 6.078082191780822e-06,
      "loss": 2.5226,
      "step": 30500
    },
    {
      "epoch": 0.7077625570776256,
      "grad_norm": 1.4857224225997925,
      "learning_rate": 5.849771689497718e-06,
      "loss": 2.5153,
      "step": 31000
    },
    {
      "epoch": 0.7191780821917808,
      "grad_norm": 2.162193775177002,
      "learning_rate": 5.621461187214612e-06,
      "loss": 2.5101,
      "step": 31500
    },
    {
      "epoch": 0.730593607305936,
      "grad_norm": 1.5602186918258667,
      "learning_rate": 5.393150684931507e-06,
      "loss": 2.5285,
      "step": 32000
    },
    {
      "epoch": 0.7420091324200914,
      "grad_norm": 1.350024700164795,
      "learning_rate": 5.1648401826484015e-06,
      "loss": 2.5284,
      "step": 32500
    },
    {
      "epoch": 0.7534246575342466,
      "grad_norm": 1.8179290294647217,
      "learning_rate": 4.9365296803652975e-06,
      "loss": 2.4965,
      "step": 33000
    },
    {
      "epoch": 0.7648401826484018,
      "grad_norm": 0.9034056067466736,
      "learning_rate": 4.708219178082193e-06,
      "loss": 2.4826,
      "step": 33500
    },
    {
      "epoch": 0.776255707762557,
      "grad_norm": 1.0709582567214966,
      "learning_rate": 4.479908675799087e-06,
      "loss": 2.5263,
      "step": 34000
    },
    {
      "epoch": 0.7876712328767124,
      "grad_norm": 1.6584223508834839,
      "learning_rate": 4.252054794520548e-06,
      "loss": 2.507,
      "step": 34500
    },
    {
      "epoch": 0.7990867579908676,
      "grad_norm": 1.454188346862793,
      "learning_rate": 4.023744292237443e-06,
      "loss": 2.5091,
      "step": 35000
    },
    {
      "epoch": 0.8105022831050228,
      "grad_norm": 1.8058758974075317,
      "learning_rate": 3.7954337899543382e-06,
      "loss": 2.5228,
      "step": 35500
    },
    {
      "epoch": 0.821917808219178,
      "grad_norm": 1.0218133926391602,
      "learning_rate": 3.567123287671233e-06,
      "loss": 2.525,
      "step": 36000
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 1.2571532726287842,
      "learning_rate": 3.3392694063926944e-06,
      "loss": 2.5099,
      "step": 36500
    },
    {
      "epoch": 0.8447488584474886,
      "grad_norm": 1.5223417282104492,
      "learning_rate": 3.1109589041095895e-06,
      "loss": 2.5095,
      "step": 37000
    },
    {
      "epoch": 0.8561643835616438,
      "grad_norm": 1.7605167627334595,
      "learning_rate": 2.882648401826484e-06,
      "loss": 2.5129,
      "step": 37500
    },
    {
      "epoch": 0.867579908675799,
      "grad_norm": 1.3698976039886475,
      "learning_rate": 2.654794520547945e-06,
      "loss": 2.5106,
      "step": 38000
    },
    {
      "epoch": 0.8789954337899544,
      "grad_norm": 1.2226336002349854,
      "learning_rate": 2.4264840182648403e-06,
      "loss": 2.5197,
      "step": 38500
    },
    {
      "epoch": 0.8904109589041096,
      "grad_norm": 1.8414067029953003,
      "learning_rate": 2.198173515981735e-06,
      "loss": 2.5359,
      "step": 39000
    },
    {
      "epoch": 0.9018264840182648,
      "grad_norm": 1.771242380142212,
      "learning_rate": 1.96986301369863e-06,
      "loss": 2.5004,
      "step": 39500
    },
    {
      "epoch": 0.91324200913242,
      "grad_norm": 1.50283682346344,
      "learning_rate": 1.7415525114155251e-06,
      "loss": 2.5071,
      "step": 40000
    },
    {
      "epoch": 0.9246575342465754,
      "grad_norm": 1.7419852018356323,
      "learning_rate": 1.51324200913242e-06,
      "loss": 2.5164,
      "step": 40500
    },
    {
      "epoch": 0.9360730593607306,
      "grad_norm": 1.3735154867172241,
      "learning_rate": 1.2849315068493152e-06,
      "loss": 2.4929,
      "step": 41000
    },
    {
      "epoch": 0.9474885844748858,
      "grad_norm": 1.4670403003692627,
      "learning_rate": 1.0566210045662101e-06,
      "loss": 2.5195,
      "step": 41500
    },
    {
      "epoch": 0.958904109589041,
      "grad_norm": 1.2671167850494385,
      "learning_rate": 8.287671232876714e-07,
      "loss": 2.4963,
      "step": 42000
    },
    {
      "epoch": 0.9703196347031964,
      "grad_norm": 1.7337955236434937,
      "learning_rate": 6.004566210045662e-07,
      "loss": 2.5137,
      "step": 42500
    },
    {
      "epoch": 0.9817351598173516,
      "grad_norm": 2.1301324367523193,
      "learning_rate": 3.721461187214612e-07,
      "loss": 2.5146,
      "step": 43000
    },
    {
      "epoch": 0.9931506849315068,
      "grad_norm": 1.7192739248275757,
      "learning_rate": 1.442922374429224e-07,
      "loss": 2.4851,
      "step": 43500
    }
  ],
  "logging_steps": 500,
  "max_steps": 43800,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6.777972699315569e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
